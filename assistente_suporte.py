import streamlit as st
import os
import glob
from dotenv import load_dotenv

# Importa√ß√µes do LangChain
from langchain_community.document_loaders import DirectoryLoader
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings, GoogleGenerativeAI
from langchain.chains import RetrievalQA

# Carrega vari√°veis de ambiente do arquivo .env
load_dotenv()

# --- CONFIGURA√á√ÉO DA P√ÅGINA STREAMLIT ---
st.set_page_config(page_title="Assistente de Suporte TI", page_icon="ü§ñ")
st.title("ü§ñ Assistente de Suporte de TI")
st.caption("Eu sou um assistente baseado nos documentos da base de conhecimento.")

# --- L√ìGICA DE CACHE PARA CARREGAR RECURSOS PESADOS ---


@st.cache_resource
def carregar_recursos():
    DIRETORIO_BASE_CONHECIMENTO = "./base_de_conhecimento"

    # Verifica√ß√£o da API Key do Google
    if "GOOGLE_API_KEY" not in os.environ:
        st.error(
            "A vari√°vel de ambiente GOOGLE_API_KEY n√£o foi encontrada. Configure-a no arquivo .env.")
        return None, None

    # Verifica se o diret√≥rio existe
    if not os.path.isdir(DIRETORIO_BASE_CONHECIMENTO):
        st.error(
            f"O diret√≥rio '{DIRETORIO_BASE_CONHECIMENTO}' n√£o foi encontrado.")
        return None, None

    with st.spinner("Analisando e carregando a base de conhecimento..."):
        # Define o loader para encontrar os arquivos
        directory_loader = DirectoryLoader(
            DIRETORIO_BASE_CONHECIMENTO,
            glob="**/*.{txt,pdf,docx,md,sql,csv,doc,png,pptx,xlsx}",
            show_progress=True,
            use_multithreading=True,
        )

        # === MELHORIA: Carregamento robusto de documentos ===
        # Em vez de carregar tudo de uma vez, carregamos um por um
        # para identificar exatamente qual arquivo pode estar com problema.
        documentos = []
        arquivos_com_erro = []

        # Usamos lazy_load() que prepara os arquivos sem carregar na mem√≥ria ainda
        for loader in directory_loader.lazy_load():
            try:
                # Tentamos carregar o conte√∫do do arquivo
                documentos.extend(loader)
            except Exception as e:
                # Se falhar, guardamos o nome do arquivo e o erro
                nome_arquivo = os.path.basename(loader.file_path)
                arquivos_com_erro.append(
                    f"Arquivo: '{nome_arquivo}' - Erro: {e}")

        # Mostra avisos na tela para cada arquivo que falhou
        if arquivos_com_erro:
            st.warning("Alguns arquivos n√£o puderam ser carregados:")
            for erro in arquivos_com_erro:
                st.write(erro)

        if not documentos:
            st.error(
                "Nenhum documento v√°lido foi lido com sucesso na base de conhecimento.")
            return None, None

        # Quebra dos textos em peda√ßos
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000, chunk_overlap=200)
        textos_divididos = text_splitter.split_documents(documentos)

        # Cria√ß√£o dos embeddings e da vector store
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        vector_store = FAISS.from_documents(textos_divididos, embeddings)

        # Cria√ß√£o da cadeia de Perguntas e Respostas
        llm = GoogleGenerativeAI(
            model="gemini-1.5-flash-latest", temperature=0.3)
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=vector_store.as_retriever(),
            return_source_documents=False
        )
        return qa_chain, len(documentos)


# --- L√ìGICA PRINCIPAL DA APLICA√á√ÉO ---
qa_chain, num_docs = carregar_recursos()

if qa_chain:
    st.success(
        f"Base de conhecimento carregada! {num_docs} documentos foram processados com sucesso.")

    # Inicializa o hist√≥rico da conversa na mem√≥ria da sess√£o
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "Ol√°! Como posso te ajudar hoje?"}]

    # Exibe as mensagens do hist√≥rico
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Captura a nova mensagem do usu√°rio
    if prompt := st.chat_input("Digite sua pergunta aqui..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # Gera e exibe a resposta do assistente
        with st.chat_message("assistant"):
            with st.spinner("Pensando..."):
                try:
                    resultado = qa_chain.invoke({"query": prompt})
                    resposta = resultado["result"]
                except Exception as e:
                    resposta = f"Desculpe, ocorreu um erro ao processar sua pergunta: {e}"
            st.markdown(resposta)

        st.session_state.messages.append(
            {"role": "assistant", "content": resposta})
else:
    st.warning(
        "O assistente n√£o est√° pronto. Verifique as mensagens de erro ou aviso acima.")
